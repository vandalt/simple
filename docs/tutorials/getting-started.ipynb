{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Getting Started\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .qmd\n",
    "      format_name: quarto\n",
    "      format_version: '1.0'\n",
    "      jupytext_version: 1.17.2\n",
    "  kernelspec:\n",
    "    display_name: Python 3\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "The goal of `simple` is to have a tiny [probablistic programming](https://en.wikipedia.org/wiki/Probabilistic_programming) language compatible with some of the most common sampling libraries.\n",
    "That way, it is easy to write a model (including all priors) for `emcee`, and then run it with `nautilus`, for example.\n",
    "\n",
    "In this tutorial, we will do a short demo by sampling a 3D normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultranest.plot import cornerplot\n",
    "import simple\n",
    "\n",
    "print(simple.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "The two key components we need to specify in a Bayesian model are the prior distribution and the likelihood function.\n",
    "Pretty much all sampling libraries require these two components, either separately for nested sampling, or combined in a posterior for MCMC.\n",
    "\n",
    "In `simple`, the prior is specified as a dictionary of `Distribution` objects.\n",
    "Most common distributions are already implemented in scipy, so the recommended approach, inspired by [nautilus](https://nautilus-sampler.readthedocs.io/en/latest/guides/priors.html), is to simply wrap scipy distributions whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, norm\n",
    "from simple.distributions import ScipyDistribution\n",
    "\n",
    "parameters = {\n",
    "    \"x1\": ScipyDistribution(uniform, -5, 10),\n",
    "    \"x2\": ScipyDistribution(uniform, 0, 10),\n",
    "    \"x3\": ScipyDistribution(norm(0, 10)),\n",
    "}\n",
    "print(\"Priors:\")\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a log-likelihood function that takes a dictionary of parameters and computes the likelihood.\n",
    "Here we don't have a dataset: we will just specify a distribution of three parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def log_likelihood(params):\n",
    "    \"\"\"Log-likelihood function for a 3D normal distribution.\"\"\"\n",
    "    p = [params[\"x1\"], params[\"x2\"], params[\"x3\"]]\n",
    "    mean = [0.0, 3.0, 2.0]\n",
    "    cov = [[1, 0.5, 0], [0.5, 1, 0], [0, 0, 1.0]]\n",
    "    return multivariate_normal.logpdf(\n",
    "        p, mean=mean, cov=cov\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to create our `simple.Model` object.\n",
    "The model needs to know what our priors and likelihood are.\n",
    "It will then wrap them to provide:\n",
    "\n",
    "- `log_prior(parameters)`: the prior distribution given a dictionary or an array of parameters\n",
    "- `log_prob(parameters)`: the posterior distribution given a dictionary or an array of parameters\n",
    "- `log_likelihood(parameters)`: a wrapper around our log-likelihood above to make it work with arrays and dictionaries\n",
    "- `prior_transform(parameters)`: a prior transform from a unit hypercube to our parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple.model import Model\n",
    "\n",
    "model = Model(parameters, log_likelihood)\n",
    "\n",
    "print(model)\n",
    "test_point = [0, 1, 0]\n",
    "print(\"Log-Prior\", model.log_prior(test_point))\n",
    "print(\"Log-Prior out of bounds\", model.log_prior([-10, 3, 0]))\n",
    "print(\"Log-likelihood\", model.log_likelihood(test_point))\n",
    "print(\"Log-posterior\", model.log_prob(test_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Checks\n",
    "\n",
    "A good thing to do before fitting any model is to check the prior.\n",
    "To sample directly from the prior, we can either pass a uniform distribution through the `prior_transform` function or sample the `log_prior()` function of our model with `emcee`.\n",
    "\n",
    "Here, we will test both approaches.\n",
    "In practice, it's probably a good idea to test your prior transform if you plan on using nested sampling and your log-prior if you plan on using MCMC.\n",
    "\n",
    "### Via prior transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "u = np.random.uniform(0, 1, size=(3, 100_000))\n",
    "prior_samples = model.prior_transform(u)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "fig = corner.corner(prior_samples.T, labels=model.keys())\n",
    "fig.suptitle(\"Prior samples via prior transform\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers=100,\n",
    "    ndim=3,\n",
    "    log_prob_fn=model.log_prior,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.array(test_point) + 1e-4 * rng.standard_normal(size=(100, 3))\n",
    "sampler.run_mcmc(p0, 1000, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = sampler.get_chain()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1)\n",
    "for i in range(3):\n",
    "    axs[i].plot(chains[:, :, i], \"k-\", alpha=0.1)\n",
    "    axs[i].set_ylabel(model.keys()[i])\n",
    "axs[-1].set_xlabel(\"Steps\")\n",
    "axs[0].set_title(\"Prior chains from emcee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chains = sampler.get_chain(discard=200, flat=True)\n",
    "fig = corner.corner(flat_chains, labels=model.keys())\n",
    "fig.suptitle(\"Prior samples via emcee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with `emcee`\n",
    "\n",
    "We can now sample the posterior with emcee. We can basically copy the previous subsection, but replace `log_prior` with `log_prob` in our sampler definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers=100,\n",
    "    ndim=3,\n",
    "    log_prob_fn=model.log_prob,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.array(test_point) + 1e-4 * rng.standard_normal(size=(100, 3))\n",
    "sampler.run_mcmc(p0, 1000, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = sampler.get_chain()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1)\n",
    "for i in range(3):\n",
    "    axs[i].plot(chains[:, :, i], \"k-\", alpha=0.1)\n",
    "    axs[i].set_ylabel(model.keys()[i])\n",
    "axs[-1].set_xlabel(\"Steps\")\n",
    "axs[0].set_title(\"Prior chains from emcee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chains = sampler.get_chain(discard=200, flat=True)\n",
    "fig = corner.corner(flat_chains, labels=model.keys())\n",
    "fig.suptitle(\"Prior samples via emcee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with `ultranest`\n",
    "\n",
    "Next, let us try and sample the model with Ultranest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultranest\n",
    "\n",
    "sampler = ultranest_patch.ReactiveNestedSampler(model.keys(), model.log_likelihood, model.prior_transform)\n",
    "\n",
    "result = sampler.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.plot_corner()\n",
    "plt.show()\n",
    "sampler.plot_run()\n",
    "plt.show()\n",
    "sampler.plot_trace()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with `nautilus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nautilus import Sampler\n",
    "\n",
    "sampler = Sampler(model.nautilus_prior(), model.log_likelihood, n_live=1000)\n",
    "sampler.run(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, log_w, log_l = sampler.posterior()\n",
    "corner.corner(\n",
    "    points, weights=np.exp(log_w), labels=model.keys(),\n",
    "    plot_datapoints=False, range=np.repeat(0.999, len(model.parameters)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_kwargs=dict( density=True)\n",
    "\n",
    "fig = corner.corner(\n",
    "    points, weights=np.exp(log_w), labels=model.keys(), color=\"purple\", hist_kwargs=hist_kwargs,\n",
    "    plot_datapoints=False, range=np.repeat(0.999, len(model.parameters))\n",
    ")\n",
    "data = np.array(result['weighted_samples']['points'])\n",
    "weights = np.array(result['weighted_samples']['weights'])\n",
    "corner.corner(\n",
    "    data, weights=weights, color=\"red\", hist_kwargs=hist_kwargs,\n",
    "    plot_datapoints=False, range=np.repeat(0.999, len(model.parameters)),\n",
    "  fig=fig,\n",
    ")\n",
    "corner.corner(\n",
    "    flat_chains, weights=np.ones(flat_chains.shape[0]), color=\"k\", hist_kwargs=hist_kwargs,\n",
    "    plot_datapoints=False,\n",
    "    fig=fig,\n",
    ")\n",
    "# Create legend elements\n",
    "from matplotlib import patches\n",
    "\n",
    "nautilus_patch = patches.Patch(color='purple', label='Nautilus')\n",
    "ultranest_patch = patches.Patch(color='red', label='Ultranest')\n",
    "emcee_patch = patches.Patch(color='k', label='Emcee')\n",
    "\n",
    "# Add legend to the figure\n",
    "fig.legend(handles=[nautilus_patch, ultranest_patch, emcee_patch],\n",
    "          loc='upper right',  # You can also use 'upper left', 'lower right', etc.\n",
    "          bbox_to_anchor=(0.98, 0.98))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
